1. collect raw text either with scraping of websites or interacting with API. Applying filters while collecting like region or product
will help in more focused study of data and also will reduce workloads
2. convert text into suitable document with indicies and build a corpus based on these indicied documents
3. compute usefulness of each word in review or text using TFIDF methods
4. categorize documents by topics. this can be achieved using Dirichlet allocation
5. determine sentiments of reviews either positive or negative or neutral
6. review the results and use visualization techniques to report the results



To represent collected data, raw text collected is transformed using normalization techniques like tokenization and case folding.
tokenization problems: tokenizinig using space or punctuation gives the. we'll we're can't
case folding problems: WHO converts to who
bag of words: represents document as set of terms : a dog bites a man and a man bites a dog considered same. it uses single-term identifier
TF term frequency of each word is calculated. 
advanced methods considers factors such as word order, context, inferences and discourse
features should also be considered such as pof tags, affixes, named entities, intonation
topic modeling is an example of creating features. it identifies cluster of words that frequently occur together and categorized as topic

it is important to create representation of corpus other than just representation of document

IC information content denotes importance of term in corpus
TFIDF is used in information retrieval and text analysis
tfidf is used on dynamic content where as corpus is used for static content

term frequency is equal to number of times term appears in document
can apply log to this frequency to enable more detail
can be normalized by dividing by number of total terms in document



stop words:
common words in language can be removed by Snowball's stop word

term freq problems: considers stand alone document as the entire world
		    imp of term is based on its freq in the document

solution is inverted document frequency (IDF): consider importance of term in all doc or in entire corpus

df: freq of term in all documents
idf: n/df

idf has problem that since n is constant, terms having same df has same idf

solution is tfidf: tf*idf

tfidf doesnt reveal connections between documents

document grouping: can be achieved by classification or clustering like support vector machines or k nearest neigh or naive bayes
good approach is topic modeling. it examines words from sets of documents,determine themes and discover how themes change over time

it has three steps:
1.uncover hidden topical patterns within a corpus
2. annotate documents according to these topics
3. use annotation to organize, search and summarize texts

topic is distribution over fixed vocabulary of words. different topics has diff dist over same vocabulary
example for topic modeling is latent dirichlet allocation(lda)
uses: document modeling,document classification,collaborative filtering

determine sentiments:nltk

gaining insights:







